#!/usr/bin/env python3
'''
Optune measure driver for SignalFx.  (C) 2018, Opsani.

use:
measure --info
measure --describe <app_id>
measure <app_id> < <measure-stdin-file.json>

This driver requires a YAML configuration file placed at a fixed location (see
CFG_FPATH constant).  See README.md for details.  Example config:

sfx:
    stream_endpoint:  <val>     # optional:  SignalFx stream API endpoint,
                                #            default https://stream.signalfx.com
    pre_cmd_async:    <val>     # optional:  async bash cmd to run before warmup
    pre_cmd:          <val>     # optional:  bash cmd to run before warmup
    post_cmd:         <val>     # optional:  bash cmd to run after measure

    metrics:
        <metric1_name>:
            flow_program:    <val>     # REQUIRED:  SignalFlow program to query metrics
            flow_immediate:  <val>     # optional:  boolean, default False
            flow_resolution: <val>     # optional:  integer ms, default None
            time_aggr:       <val>     # optional:  aggr method for space aggregates
                                       #            default 'avg'
            space_aggr:      <val>     # optional:  aggr method for values at a
                                       #            given time; default 'avg'
            unit:            <val>     # optional:  unit of metric, e.g 'req/sec'
        <metric2_name>:
            flow_program:  ...

'''

import json
import numbers
import os
import signal
import subprocess
import sys
import time
import yaml

import signalfx

from measure import Measure, ST_FAILED

DESC                 = 'SignalFx measure driver for Opsani Optune'
VERSION              = '1.0.0'
HAS_CANCEL           = True
PROGRESS_INTERVAL    = 30
CFG_FPATH            = './config.yaml'

DFLT_WARMUP          = 0
DFLT_DURATION        = 120
DFLT_TIME_AGGR       = 'avg'
DFLT_SPACE_AGGR      = 'avg'
DFLT_STREAM_ENDPOINT = 'https://stream.signalfx.com'
DFLT_FLOW_IMMEDIATE  = False
DFLT_FLOW_RESOLUTION = None
DFLT_NODATA_PCT_OK   = 0.0

AGGR_OPERATORS       = ('avg', 'max', 'min', 'sum')
API_KEY_FPATH        = '/etc/optune-sfx-auth/api_key'
API_KEY_ENV_VAR      = 'OPTUNE_SFX_API_KEY'

class SignalFx(Measure):

    # overwrites super
    def describe(self):

        # parse driver config and construct metrics describe response
        self._parse_config()
        description = {}
        for metric_name, metric in self.cfg['metrics'].items():
            metric_spec = {}
            if metric.get('unit') is not None:
                metric_spec['unit'] = metric.get('unit')
            description[metric_name] = metric_spec

        return description

    # overwrites super
    def handle_cancel(self, signal, frame):
        err = 'Exiting due to signal: {}'.format(signal)
        self.print_measure_error(err, ST_FAILED)

        # terminate pre_cmd_async, if any
        if hasattr(self, 'proc_async'):
            os.killpg(os.getpgid(self.proc_async.pid), signal.SIGTERM)

        sys.exit(3)

    # overwrites super
    def measure(self):

        # parse and valcheck:  control configuration
        assert 'control' in self.input_data, 'Input data missing control configuration'
        control  = self.input_data['control']
        warmup   = int(control.get('warmup', DFLT_WARMUP))
        duration = int(control.get('duration', DFLT_DURATION))
        assert warmup >= 0 and duration >= 0, \
            'Both warmup {} and duration {} must be non-negative'.format(warmup, duration)
        nodata_pct_ok = control.get('userdata',{}).get('nodata_pct_ok', DFLT_NODATA_PCT_OK)
        self.t_sleep = warmup + duration

        # parse and valcheck:  driver config
        self._parse_config()

        # query sfx:  verify signalflow program(s) are valid
        self._init_sfx()
        end_time = time.time()
        for metric_name, metric in self.cfg['metrics'].items():
            qval = self._query_sfx_metric(metric['flow_program'],
                metric['flow_immediate'], metric['flow_resolution'],
                metric['time_aggr'], metric['space_aggr'], end_time,
                duration, nodata_pct_ok)
            self._nfy('Initial value for query {}:  {}'.format(
                metric['flow_program'], qval))

        # execute pre_cmd_async, if any
        if self.cfg['pre_cmd_async'] is not None:
            cmd = self.cfg['pre_cmd_async'].format(**control)
            self.proc_async = self._run_command_async(cmd)

        # execute pre_cmd, if any
        if self.cfg['pre_cmd'] is not None:
            self._run_command(self.cfg['pre_cmd'], self.cfg['pre_cmd_tout'],
                pre=True)

        # sleep
        self._nfy('Sleeping for {:d} seconds ({:d} warmup + {:d} duration)'.format(
            self.t_sleep, warmup, duration))
        time.sleep(self.t_sleep)

        # query sfx:  measure
        measurement = {}
        annotations = {}
        end_time = time.time()
        for metric_name, metric in self.cfg['metrics'].items():
            qval = self._query_sfx_metric(metric['flow_program'],
                metric['flow_immediate'], metric['flow_resolution'],
                metric['time_aggr'], metric['space_aggr'], end_time,
                duration, nodata_pct_ok)
            metric_spec = {
                'value': qval,
                'annotation': metric['flow_program'],
            }
            if metric.get('unit') is not None:
                metric_spec['unit'] = metric.get('unit')
            measurement[metric_name] = metric_spec

        # execute post_cmd, if any
        if self.cfg['post_cmd'] is not None:
            self._run_command(self.cfg['post_cmd'], self.cfg['post_cmd_tout'],
                pre=False)

        # terminate pre_cmd_async, if any
        if hasattr(self, 'proc_async'):
            os.killpg(os.getpgid(self.proc_async.pid), signal.SIGTERM)

        return measurement, annotations

    # overwrites super:  update progress before printing it
    def print_progress(
            self,
            message=None,
            msg_index=None,
            stage=None,
            stageprogress=None):

        # update progress based on how much time has elapsed
        if hasattr(self, 't_sleep'):
            t_taken = time.time() - self.t_measure_start
            self.progress = int(min(100.0, 100.0*((t_taken)/self.t_sleep)))
            super().print_progress(message, msg_index, stage, stageprogress)

    # helper:  initialize sfx api module
    def _init_sfx(self):

        # as required:  read sfx_api_key from file (e.g., from k8s secret volume
        # mounted file) or from env variable
        if not hasattr(self, 'sfx_api_key'):
            if os.path.isfile(API_KEY_FPATH):
                with open(API_KEY_FPATH, 'r') as f:
                    self.sfx_api_key = f.read().rstrip()
            elif os.getenv(API_KEY_ENV_VAR, None) is not None:
                self.sfx_api_key = os.getenv(API_KEY_ENV_VAR)
            else:
                raise Exception('SignalFx API key does not exist either as the ' +
                    'file {} or as the env variable {}'.format(API_KEY_FPATH,
                    API_KEY_ENV_VAR))

        # initialize sfx api module
        try:
            self.sfx = signalfx.SignalFx(stream_endpoint=self.cfg['stream_endpoint'])
        except Exception as e:
            raise Exception(
                'Failed to initialize SignalFx API module. Error: {}'.format(str(e)))

    # helper:  query sfx metrics over last duration seconds and return
    # value computed from aggregating in time and space
    def _query_sfx_metric(self, program, immediate, resolution, time_aggr,
        space_aggr, end_time, duration, nodata_pct_ok):

        n_data_msg = 0
        n_nodata_msg = 0
        space_aggr_vals = []
        with self.sfx.signalflow(self.sfx_api_key) as flow:

            # execute SignalFlow program, obtaining a stream object:
            stop_ms = int(end_time) * 1000
            start_ms = stop_ms - (duration * 1000)
            try:
                computation = flow.execute(program, start=start_ms, stop=stop_ms,
                    immediate=immediate, resolution=resolution)
            except Exception as e:
                raise Exception(
                    'Failed to query SignalFx for program {}. Error: {}'.format(
                    program, str(e)))

            # process messages in computation stream
            for msg in computation.stream():
                if isinstance(msg, signalfx.signalflow.messages.DataMessage):
                    space_vals = [x for x in list(msg.data.values()) if isinstance(
                        x, numbers.Number)]
                    if len(space_vals) == 0:
                        n_nodata_msg += 1
                    else:
                        n_data_msg += 1
                        space_aggr_vals.append(self._aggregate_values(space_vals,
                            space_aggr))

        # verify percent of empty data messages is below configured threshold
        n_msg = n_data_msg + n_nodata_msg
        nodata_pct = 100.0 * (n_nodata_msg / n_msg)
        self._nfy('SignalFlow stream: {} data msgs, {} nodata msgs, {:.2f}% empty for program {}'.format(
            n_data_msg, n_nodata_msg, nodata_pct, program))
        assert nodata_pct <= nodata_pct_ok, \
            'SignalFlow stream:  {:.2f}% of {} data msgs are empty ({:.2f}% is ok) for program {}'.format(
            nodata_pct, n_msg, nodata_pct_ok, program)
        assert len(space_aggr_vals) > 0, \
            'SignalFx query for program {} returned no time series metrics data'.format(
            program)

        # aggregate in time and return value
        return self._aggregate_values(space_aggr_vals, time_aggr)

    # helper:  return an aggregate value computed from an input list of values
    def _aggregate_values(self, vals, aggr):
        if aggr == 'avg':
            return sum(vals) / float(len(vals))
        if aggr == 'max':
            return max(vals)
        if aggr == 'min':
            return min(vals)
        if aggr == 'sum':
            return sum(vals)
        raise Exception('Unexpected aggregation method {}'.format(aggr))

    # helper:  run a Bash shell command and raise an Exception on failure
    # note:  if cmd is a string, this supports shell pipes, environment variable
    # expansion, etc.  The burden of safety is entirely on the user.
    def _run_command(self, cmd, tout=None, pre=True):
        cmd_type = 'Pre-command' if pre else 'Post-command'
        res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
            shell=True, timeout=tout, executable='/bin/bash') # nosec
        msg = "cmd '{}', exit code {}, stdout {}, stderr {}".format(cmd,
            res.returncode, res.stdout, res.stderr)
        assert res.returncode == 0, '{} failed:  {}'.format(cmd_type, msg)
        self._nfy('{}:  {}'.format(cmd_type, msg))

    # helper:  run a Bash shell command with stdout/stderr directed to /dev/null
    # and return the popen object
    # note:  if cmd is a string, this supports shell pipes, environment variable
    # expansion, etc.  The burden of safety is entirely on the user.
    def _run_command_async(self, cmd):
        proc = subprocess.Popen(cmd, stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL, shell=True, executable='/bin/bash', # nosec
            preexec_fn=os.setpgrp)
        self._nfy('Pre-command async:  {}'.format(cmd))
        return proc

    # helper:  parse driver configuration from config file
    def _parse_config(self):

        # load YAML config file (raise any unhandled exception)
        try:
            fo = open(CFG_FPATH)
            desc = yaml.safe_load(fo)
        except IOError as e:
            raise Exception('Cannot read configuration from {}:  {}'.format(
                CFG_FPATH, e.strerror))
        except yaml.error.YAMLError as e:
            raise Exception('Syntax error in {}:  {}'.format(CFG_FPATH, str(e)))
        fo.close()

        # parse descriptor
        sfx = desc.get('sfx')
        assert isinstance(sfx, dict), 'No sfx config in {}'.format(CFG_FPATH)
        self.cfg = {}
        self.cfg['stream_endpoint'] = sfx.get('stream_endpoint', DFLT_STREAM_ENDPOINT)
        self.cfg['pre_cmd_async']   = sfx.get('pre_cmd_async')
        self.cfg['pre_cmd']         = sfx.get('pre_cmd')
        self.cfg['post_cmd']        = sfx.get('post_cmd')
        self.cfg['pre_cmd_tout']    = sfx.get('pre_cmd_tout')
        self.cfg['post_cmd_tout']   = sfx.get('post_cmd_tout')

        # parse descriptor:  metrics
        metrics = sfx.get('metrics',{})
        assert isinstance(metrics, dict) and len(metrics) > 0, \
            'No metrics config in {}'.format(CFG_FPATH)
        for metric_name, metric in metrics.items():
            assert 'flow_program' in metric, \
                'No flow_program configured for metric {} in {}'.format(
                metric_name, CFG_FPATH)
            metric['flow_immediate'] = metric.get('flow_immediate',
                DFLT_FLOW_IMMEDIATE)
            metric['flow_resolution'] = metric.get('flow_resolution',
                DFLT_FLOW_RESOLUTION)
            metric['time_aggr'] = metric.get('time_aggr', DFLT_TIME_AGGR)
            metric['space_aggr'] = metric.get('space_aggr', DFLT_SPACE_AGGR)
            assert metric['time_aggr'] in AGGR_OPERATORS, \
                'Unknown time_aggr {} for metric {} in {}'.format(
                metric['time_aggr'], metric_name, CFG_FPATH)
            assert metric['space_aggr'] in AGGR_OPERATORS, \
                'Unknown space_aggr {} for metric {} in {}'.format(
                metric['space_aggr'], metric_name, CFG_FPATH)
        self.cfg['metrics'] = metrics

    # helper:  print a msg to servo stderr (debug log) and log to the Optune
    # back-end on stdout (JSON formatted)
    def _nfy(self, msg):
        self.debug(msg)
        data = { 'progress': 0, 'message': msg }
        print(json.dumps(data), flush=True)


if __name__ == '__main__':
    sfx = SignalFx(VERSION, DESC, HAS_CANCEL, PROGRESS_INTERVAL)
    sfx.run()
