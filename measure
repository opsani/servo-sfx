#!/usr/bin/env python3
'''
Optune measure driver for SignalFx.  (C) 2018, Opsani.

use:
measure --info
measure --describe <app_id>
measure <app_id> < <measure-stdin-file.json>

This driver requires a YAML configuration file placed at a fixed location (see
CFG_FPATH constant).  See README.md for details.  Example config:

sfx:
    stream_endpoint:  <val>     # optional:  SignalFx stream API endpoint,
                                #            default https://stream.signalfx.com
    pre_cmd_async:    <val>     # optional:  async bash cmd to run before warmup
    pre_cmd:          <val>     # optional:  bash cmd to run before warmup
    post_cmd:         <val>     # optional:  bash cmd to run after measure

    metrics:
        <metric1_name>:
            flow_program:    <val>     # REQUIRED:  SignalFlow program to query metrics
            flow_immediate:  <val>     # optional:  boolean, default False
            flow_resolution: <val>     # optional:  integer ms, default None
            unit:            <val>     # optional:  unit of metric, e.g 'req/sec'
        <metric2_name>:
            flow_program:  ...

'''

import json
import os
import signal
import subprocess
import sys
import time
from collections import OrderedDict

import yaml

import signalfx

from measure import Measure, ST_FAILED

DESC                 = 'SignalFx measure driver for Opsani Optune'
VERSION              = '1.0.0'
HAS_CANCEL           = True
PROGRESS_INTERVAL    = 30
CFG_FPATH            = './config.yaml'

DFLT_WARMUP          = 0
DFLT_DURATION        = 120
DFLT_STREAM_ENDPOINT = 'https://stream.signalfx.com'
DFLT_FLOW_IMMEDIATE  = False
DFLT_FLOW_RESOLUTION = None

API_KEY_FPATH        = '/etc/optune-sfx-auth/api_key'
API_KEY_ENV_VAR      = 'OPTUNE_SFX_API_KEY'

class SignalFx(Measure):

    # overwrites super
    def describe(self):

        # parse driver config and construct metrics describe response
        self._parse_config()
        description = {}
        for metric_name, metric in self.cfg['metrics'].items():
            metric_spec = {}
            if metric.get('unit') is not None:
                metric_spec['unit'] = metric.get('unit')
            description[metric_name] = metric_spec

        return description

    # overwrites super
    def handle_cancel(self, signal, frame):
        err = 'Exiting due to signal: {}'.format(signal)
        self.print_measure_error(err, ST_FAILED)

        # terminate pre_cmd_async, if any
        if hasattr(self, 'proc_async'):
            os.killpg(os.getpgid(self.proc_async.pid), signal.SIGTERM)

        sys.exit(3)

    # overwrites super
    def measure(self):
        # parse and valcheck:  control configuration
        assert 'control' in self.input_data, 'Input data missing control configuration'
        control  = self.input_data['control']
        warmup   = int(control.get('warmup', DFLT_WARMUP))
        duration = int(control.get('duration', DFLT_DURATION))
        assert warmup >= 0 and duration >= 0, \
            'Both warmup {} and duration {} must be non-negative'.format(warmup, duration)
        self.t_sleep = warmup + duration

        # parse and valcheck:  driver config
        self._parse_config()

        # query sfx:  verify signalflow program(s) are valid
        self._init_sfx()
        end_time = time.time()
        for metric_name, metric in self.cfg['metrics'].items():
            qval = self._query_sfx_metric(metric['flow_program'], metric['flow_immediate'], metric['flow_resolution'],
                                          end_time, duration)
            self._nfy('Initial value for query {}:  {}'.format(
                metric['flow_program'], qval))

        # execute pre_cmd_async, if any
        if self.cfg['pre_cmd_async'] is not None:
            self.proc_async = self._run_command_async(self.cfg['pre_cmd_async'])

        # execute pre_cmd, if any
        if self.cfg['pre_cmd'] is not None:
            self._run_command(self.cfg['pre_cmd'], self.cfg['pre_cmd_tout'],
                pre=True)

        # sleep
        self._nfy('Sleeping for {:d} seconds ({:d} warmup + {:d} duration)'.format(
            self.t_sleep, warmup, duration))
        time.sleep(self.t_sleep)

        # query sfx:  measure
        measurement = {}
        annotations = {}
        end_time = time.time()
        for metric_name, metric in self.cfg['metrics'].items():
            qval = self._query_sfx_metric(metric['flow_program'], metric['flow_immediate'], metric['flow_resolution'],
                                          end_time, duration)
            metric_spec = {
                'values': qval,
                'annotation': metric['flow_program'],
            }
            if metric.get('unit') is not None:
                metric_spec['unit'] = metric.get('unit')
            measurement[metric_name] = metric_spec

        # execute post_cmd, if any
        if self.cfg['post_cmd'] is not None:
            self._run_command(self.cfg['post_cmd'], self.cfg['post_cmd_tout'],
                pre=False)

        # terminate pre_cmd_async, if any
        if hasattr(self, 'proc_async'):
            os.killpg(os.getpgid(self.proc_async.pid), signal.SIGTERM)

        return measurement, annotations

    # overwrites super:  update progress before printing it
    def print_progress(
            self,
            message=None,
            msg_index=None,
            stage=None,
            stageprogress=None):
        # update progress based on how much time has elapsed
        if hasattr(self, 't_sleep'):
            t_taken = time.time() - self.t_measure_start
            self.progress = int(min(100.0, 100.0*((t_taken)/self.t_sleep)))
            super().print_progress(message, msg_index, stage, stageprogress)

    # helper:  initialize sfx api module
    def _init_sfx(self):
        # as required:  read sfx_api_key from file (e.g., from k8s secret volume
        # mounted file) or from env variable
        if not hasattr(self, 'sfx_api_key'):
            if os.path.isfile(API_KEY_FPATH):
                with open(API_KEY_FPATH, 'r') as f:
                    self.sfx_api_key = f.read().rstrip()
            elif os.getenv(API_KEY_ENV_VAR, None) is not None:
                self.sfx_api_key = os.getenv(API_KEY_ENV_VAR)
            else:
                raise Exception('SignalFx API key does not exist either as the ' +
                    'file {} or as the env variable {}'.format(API_KEY_FPATH,
                    API_KEY_ENV_VAR))

        # initialize sfx api module
        try:
            self.sfx = signalfx.SignalFx(stream_endpoint=self.cfg['stream_endpoint'])
        except Exception as e:
            raise Exception(
                'Failed to initialize SignalFx API module. Error: {}'.format(str(e)))

    # helper:  query sfx metrics over last duration seconds
    def _query_sfx_metric(self, program, immediate, resolution, end_time, duration):
        with self.sfx.signalflow(self.sfx_api_key) as flow:
            # execute SignalFlow program, obtaining a stream object:
            stop_ms = int(end_time) * 1000
            start_ms = stop_ms - (duration * 1000)
            try:
                computation = flow.execute(program, start=start_ms, stop=stop_ms,
                    immediate=immediate, resolution=resolution)
            except Exception as e:
                raise Exception(
                    'Failed to query SignalFx for program {}. Error: {}'.format(
                    program, str(e)))

            metadata_messages = {}
            data_messages = []

            # process messages in program execution/computation stream
            for msg in computation.stream():
                if isinstance(msg, signalfx.signalflow.messages.MetadataMessage):
                    metadata_messages[msg.tsid] = msg
                if isinstance(msg, signalfx.signalflow.messages.DataMessage):
                    # only add messages that have datapoints in it
                    if msg.data:
                        data_messages.append(msg)

            # Read more about 'sf_key' at https://developers.signalfx.com/signalflow_analytics/rest_api_messages/stream_messages_specification.html#_metadata_messages
            inapplicable_sf_keys = ['sf_metric', 'sf_originatingMetric', 'computationId']
            inst_ret = OrderedDict()

            # Go through each metadata message that represents a timeseries for requested program and define its id
            for tsid, msg in metadata_messages.items():
                id_keys = sorted(set(msg.properties['sf_key']) - set(inapplicable_sf_keys))
                if id_keys:
                    # Group id represents an identification for a particular timeseries set of a particular metric
                    # Metadata message will always have `properties` key, which is an object that contains particular timeseries meta properties.
                    # Keys from `sf_key` meta property will always be present in metadata object `properties` of the message.
                    # If there's no other timeseries identifying keys present in `sf_key` list, then use timeseries id which is SignalFx's internal identifier.
                    # To have actual instance id as an identifier provide grouping within particular metric's program based on the metric property that represents instance id.
                    group_id = '   '.join(map(lambda key: '{}:{}'.format(key, msg.properties[key]), id_keys))
                else:
                    group_id = tsid
                inst_ret[tsid] = {'data': [], 'id': str(group_id)}

            # Populate resulting timeseries with datapoints acquired from incoming data messages using SignalFx timeseries id (tsid) that we got from incoming metadata messages.
            # Expected resulting data structure to be sent to the backend: [{"data": [[timestamp, value], …], "id": group_id}, …]
            for msg in data_messages:
                for tsid, value in msg.data.items():
                    inst = inst_ret[tsid]
                    datapoint = [msg.logical_timestamp_ms, value]
                    inst['data'].append(datapoint)

        return list(inst_ret.values())

    # helper:  run a Bash shell command and raise an Exception on failure
    # note:  if cmd is a string, this supports shell pipes, environment variable
    # expansion, etc.  The burden of safety is entirely on the user.
    def _run_command(self, cmd, tout=None, pre=True):
        cmd_type = 'Pre-command' if pre else 'Post-command'
        res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
            shell=True, timeout=tout, executable='/bin/bash')
        msg = "cmd '{}', exit code {}, stdout {}, stderr {}".format(cmd,
            res.returncode, res.stdout, res.stderr)
        assert res.returncode == 0, '{} failed:  {}'.format(cmd_type, msg)
        self._nfy('{}:  {}'.format(cmd_type, msg))

    # helper:  run a Bash shell command with stdout/stderr directed to /dev/null
    # and return the popen object
    def _run_command_async(self, cmd):
        proc = subprocess.Popen(cmd, stdin=subprocess.DEVNULL, stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL, shell=True, executable='/bin/bash',
            preexec_fn=os.setpgrp)
        self._nfy('Pre-command async:  {}'.format(cmd))
        return proc

    # helper:  parse driver configuration from config file
    def _parse_config(self):

        # load YAML config file (raise any unhandled exception)
        try:
            fo = open(CFG_FPATH)
            desc = yaml.load(fo)
        except IOError as e:
            raise Exception('Cannot read configuration from {}:  {}'.format(
                CFG_FPATH, e.strerror))
        except yaml.error.YAMLError as e:
            raise Exception('Syntax error in {}:  {}'.format(CFG_FPATH, str(e)))
        fo.close()

        # parse descriptor
        sfx = desc.get('sfx')
        assert isinstance(sfx, dict), 'No sfx config in {}'.format(CFG_FPATH)
        self.cfg = {}
        self.cfg['stream_endpoint'] = sfx.get('stream_endpoint', DFLT_STREAM_ENDPOINT)
        self.cfg['pre_cmd_async']   = sfx.get('pre_cmd_async')
        self.cfg['pre_cmd']         = sfx.get('pre_cmd')
        self.cfg['post_cmd']        = sfx.get('post_cmd')
        self.cfg['pre_cmd_tout']    = sfx.get('pre_cmd_tout')
        self.cfg['post_cmd_tout']   = sfx.get('post_cmd_tout')

        # parse descriptor:  metrics
        metrics = sfx.get('metrics',{})
        assert isinstance(metrics, dict) and len(metrics) > 0, \
            'No metrics config in {}'.format(CFG_FPATH)
        for metric_name, metric in metrics.items():
            assert 'flow_program' in metric, \
                'No flow_program configured for metric {} in {}'.format(
                metric_name, CFG_FPATH)
            metric['flow_immediate'] = metric.get('flow_immediate',
                DFLT_FLOW_IMMEDIATE)
            metric['flow_resolution'] = metric.get('flow_resolution',
                DFLT_FLOW_RESOLUTION)
        self.cfg['metrics'] = metrics

    # helper:  print a msg to servo stderr (debug log) and log to the Optune
    # back-end on stdout (JSON formatted)
    def _nfy(self, msg):
        self.debug(msg)
        data = { 'progress': 0, 'message': msg }
        print(json.dumps(data), flush=True)


if __name__ == '__main__':
    sfx = SignalFx(VERSION, DESC, HAS_CANCEL, PROGRESS_INTERVAL)
    sfx.run()
